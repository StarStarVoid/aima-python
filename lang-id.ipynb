{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from text import *\n",
    "from decimal import Decimal\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def sentences_from_dataset(file_path):\n",
    "    \"\"\" Read text from a language file and extract sentences. \"\"\"\n",
    "\n",
    "    with open(file_path, 'r') as myfile:\n",
    "        lines=myfile.readlines()\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    for line in lines:\n",
    "        number, sentence = line.split('\\t')\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def clean_text(sentence):\n",
    "    \"\"\" Lowercase the sentence, replace newline character with space and remove punctuation. \"\"\"\n",
    "\n",
    "    sentence = sentence.replace('\\n', ' ')\n",
    "    sentence = sentence.lower()\n",
    "    return ''.join(l for l in sentence if l not in string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract the sentences from our training set for each lanuage and clean them up. ** I am yet to add a description of how the data is stored.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_sentences = sentences_from_dataset('data/eng.txt')\n",
    "french_sentences = sentences_from_dataset('data/fra.txt')\n",
    "ind_sentences = sentences_from_dataset('data/ind.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clean each of these sentences. And join them with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_sentences = ' '.join([clean_text(s) for s in english_sentences])\n",
    "french_sentences = ' '.join([clean_text(s) for s in french_sentences])\n",
    "ind_sentences = ' '.join([clean_text(s) for s in ind_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use some test text and clean it up. The final notebook will have more of these possibly in a dict with keys so we can count how many we got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1_ind = '''Setiap orang berhak mendapat pendidikan. Pendidikan harus gratis, setidak-tidaknya untuk tingkat sekolah rendah dan pendidikan dasar. Pendidikan rendah harus diwajibkan. Pendidikan teknik dan jurusan secara umum harus terbuka bagi semua orang, dan pengajaran tinggi harus secara adil dapat diakses oleh semua orang, berdasarkan kepantasan.'''\n",
    "test2_eng = '''In the traditional sense a hacker is a person who is extremely interested in exploring the things and recondite workings of any computer system. Most often, hackers are the expert programmers.'''\n",
    "\n",
    "test1_ind = clean_text(test1_ind)\n",
    "test2_eng = clean_text(test2_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation 1: Using modified version of NgramTextModel in text.py\n",
    "\n",
    "The current implementation is supposed to be used with words. We can use it for letter n-grams by treating each character as a word. We can create a char n-gram for each language in our training set. I have introduced a slight modification in the constructor to support Laplacian smoothing. You can compare the orignal and the modified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%psource NgramTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New Version\n",
    "\n",
    "class NgramTextModel(CountingProbDist):\n",
    "\n",
    "    \"\"\"This is a discrete probability distribution over n-tuples of words.\n",
    "    You can add, sample or get P[(word1, ..., wordn)]. The method P.samples(n)\n",
    "    builds up an n-word sequence; P.add and P.add_sequence add data.\"\"\"\n",
    "\n",
    "    def __init__(self, n, observation_sequence=[], default=1):\n",
    "        # In addition to the dictionary of n-tuples, cond_prob is a\n",
    "        # mapping from (w1, ..., wn-1) to P(wn | w1, ... wn-1)\n",
    "        CountingProbDist.__init__(self, default=default)\n",
    "        self.n = n\n",
    "        self.cond_prob = defaultdict()\n",
    "        self.add_sequence(observation_sequence)\n",
    "\n",
    "    # __getitem__, top, sample inherited from CountingProbDist\n",
    "    # Note they deal with tuples, not strings, as inputs\n",
    "\n",
    "    def add(self, ngram):\n",
    "        \"\"\"Count 1 for P[(w1, ..., wn)] and for P(wn | (w1, ..., wn-1)\"\"\"\n",
    "        CountingProbDist.add(self, ngram)\n",
    "        if ngram[:-1] not in self.cond_prob:\n",
    "            self.cond_prob[ngram[:-1]] = CountingProbDist()\n",
    "        self.cond_prob[ngram[:-1]].add(ngram[-1])\n",
    "\n",
    "    def add_sequence(self, words):\n",
    "        \"\"\"Add each of the tuple words[i:i+n], using a sliding window.\n",
    "        Prefix some copies of the empty word, '', to make the start work.\"\"\"\n",
    "        n = self.n\n",
    "        words = ['', ] * (n - 1) + words\n",
    "        for i in range(len(words) - n):\n",
    "            self.add(tuple(words[i:i + n]))\n",
    "\n",
    "    def samples(self, nwords):\n",
    "        \"\"\"Build up a random sample of text nwords words long, using\n",
    "        the conditional probability given the n-1 preceding words.\"\"\"\n",
    "        n = self.n\n",
    "        nminus1gram = ('',) * (n-1)\n",
    "        output = []\n",
    "        for i in range(nwords):\n",
    "            if nminus1gram not in self.cond_prob:\n",
    "                nminus1gram = ('',) * (n-1)  # Cannot continue, so restart.\n",
    "            wn = self.cond_prob[nminus1gram].sample()\n",
    "            output.append(wn)\n",
    "            nminus1gram = nminus1gram[1:] + (wn,)\n",
    "        return ' '.join(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_ng = NgramTextModel(3, list(english_sentences), default=1)\n",
    "f_ng = NgramTextModel(3, list(french_sentences),default=1)\n",
    "i_ng = NgramTextModel(3, list(ind_sentences),default=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the most commong occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_ng.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are treating P(l) the prior for language as uniform we ignore this term in our calculations. We can evaluate the probability of P(l | test_text) for each language by multiplying the probability of each n-gram in the test_text. We are dealing with tri-grams here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_ngrams(text, n):\n",
    "    \"\"\" List of ngram tuples that work well with our NGramTextModel \"\"\"\n",
    "    return [tuple(text[i:i+n]) for i in range(len(text)-1)]\n",
    "\n",
    "test_1_ngrams = create_ngrams(test1_ind, 3)\n",
    "print(test_1_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **P(lang | text)**. We use decimal for precision reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import product\n",
    "\n",
    "prob_lang_dict = dict(\n",
    "english = product([Decimal(e_ng[trigram]) for trigram in test_1_ngrams]),\n",
    "indonesian = product([Decimal(i_ng[trigram]) for trigram in test_1_ngrams]),\n",
    "french = product([Decimal(f_ng[trigram]) for trigram in test_1_ngrams]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to compare these to predict the language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(prob_lang_dict, key=prob_lang_dict.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above workflow nicely wrapped up into a Class. I have left out the procedure of getting the data and cleaning because it may vary depending on the source of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageID:\n",
    "    \"\"\" training_corpus should be a dict of language name as keys and cleaned up strings of text as values. \"\"\"\n",
    "\n",
    "    def __init__(self, training_corpus, n=3, smoothing_factor=1):\n",
    "        self.n = n\n",
    "        self.languages = training_corpus.keys()\n",
    "        self.training_ngram_models = {language: NgramTextModel(n, list(text), smoothing_factor) \n",
    "                                   for language, text in training_corpus.items()}\n",
    "\n",
    "    def create_ngrams(self, text):\n",
    "        \"\"\" List of ngram tuples that work well with our NGramTextModel \"\"\"\n",
    "        return [tuple(text[i:i+self.n]) for i in range(len(text)-1)]\n",
    "    \n",
    "    def calculate_posterior(self, language, test_ngrams):\n",
    "        \"\"\" Posterior sans the P(l) term by taking product of prob of each ngram \"\"\"\n",
    "        return product([Decimal(self.training_ngram_models[language][ngram]) for ngram in test_ngrams])\n",
    "\n",
    "    def predict(self, text):\n",
    "        test_ngrams = self.create_ngrams(text)\n",
    "        prob_lang_dict = {language: self.calculate_posterior(language, test_ngrams) for language in self.languages}\n",
    "        return max(prob_lang_dict, key=prob_lang_dict.get) # Key with max prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor = LanguageID(dict(english=english_sentences, indonesian=ind_sentences, french=french_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor.predict(test2_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from utils import weighted_sampler, weighted_sample_with_replacement, isclose\n",
    "\n",
    "def normalize(dist):\n",
    "    \"\"\"Multiply each number by a constant such that the sum is 1.0\"\"\"\n",
    "    if isinstance(dist, dict):\n",
    "        print(dist)\n",
    "        total = sum(dist.values())\n",
    "        for key in dist:\n",
    "            dist[key] = dist[key] / total\n",
    "            assert 0 <= dist[key] <= 1, \"Probabilities must be between 0 and 1.\"\n",
    "        return dist\n",
    "    total = sum(dist)\n",
    "    return [(n / total) for n in dist]\n",
    "\n",
    "class ProbDist(dict):\n",
    "    \"\"\"A Probability Distribution is an {outcome: probability} mapping.\n",
    "    The values are normalized to sum to 1.\n",
    "    ProbDist(0.75) is an abbreviation for ProbDist({T: 0.75, F: 0.25}).\"\"\"\n",
    "    def __init__(self, mapping=(), **kwargs):\n",
    "        if isinstance(mapping, float):\n",
    "            mapping = {T: mapping, F: 1 - mapping}\n",
    "        self.update(mapping, **kwargs)\n",
    "        normalize(self)\n",
    "\n",
    "    def sample(self, n):\n",
    "        return weighted_sample_with_replacement(list(self.keys()), list(self.values()), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DefaultProbDist(ProbDist):\n",
    "    def __init__(self, default_value, mapping=(), **kwargs):\n",
    "        if isinstance(mapping, float):\n",
    "            mapping = {True: mapping, False: 1 - mapping}\n",
    "        self.update(mapping, **kwargs)\n",
    "        self[None] = default_value\n",
    "        normalize(self)\n",
    "       \n",
    "    def __missing__(self, key): \n",
    "        \"If we haven't seen key before, use default_value as the probability and re-normalize.\"\n",
    "        self[key] = self.default_value\n",
    "        self.normalize()\n",
    "        return self[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NEW IMPLEMENTATION WHICH BACKS OFF IF EVIDENCE IS NOT FOUND\n",
    "\n",
    "class NewNgramModel(DefaultProbDist):\n",
    "    \"\"\" Avoids Counting Prob Dist. Uses filtering instead of creating\n",
    "    a cond prob dist so as to handle cases better when nothing is found\n",
    "    meeting the evidence. example we check for \"qw\" as evidence in trigrams.\n",
    "    A possible match is \"qwe\" if nothing is found with evidence \"qw\" we fall\n",
    "    back to \"w\" as evidence.\"\"\"\n",
    "\n",
    "    def __init__(self, n, default_value, observation_sequence=[]):\n",
    "        self.n = n\n",
    "        self.element_counts = Counter(observation_sequence)\n",
    "        ngram_counts = self.generate_ngram_counts(observation_sequence, n)\n",
    "        super().__init__(default_value, ngram_counts)\n",
    "\n",
    "    def generate_ngram_counts(self, observation_sequence, n):\n",
    "        \"Ngrams generated using sliding window\"\n",
    "        return Counter(tuple(observation_sequence[i:i + n])\n",
    "                       for i in range(len(observation_sequence) - 1))\n",
    "\n",
    "    def sample_text(self, sample_length):\n",
    "        \"\"\" If sample_length is less than n we sample according to frequency\n",
    "        of words/letters (elements). In case the last n-1 elements which act\n",
    "        as evidence are not found we again sample after decreasing the evidence\n",
    "        to n-2 and so on.\"\"\"\n",
    "\n",
    "        def ngram_for_decreasing_evidence(output):\n",
    "            e_size = self.n - 1   # Evidence Size\n",
    "            ngrams_matching_evidence = []\n",
    "            while len(ngrams_matching_evidence) == 0:  # No matching ngrams yet.\n",
    "                if e_size == 1: \n",
    "                    return []  # Return none when no ngram supports evidence.\n",
    "\n",
    "                evidence = output[-(e_size):]  # Last elements from currrently generated output\n",
    "\n",
    "                # Next we filter matching the evidence with last \n",
    "                # e_size characters of the ngrams sans the last element\n",
    "\n",
    "                ngrams_matching_evidence = [ngram for ngram in self\n",
    "                                            if ngram is not None and list(ngram[-(e_size+1):-1]) == evidence]\n",
    "\n",
    "                e_size -= 1  # Decrease evidence to support a smaller ngram of e_size if not found\n",
    "\n",
    "            return ngrams_matching_evidence\n",
    "\n",
    "        output = []\n",
    "        while len(output) != sample_length:\n",
    "            if len(output) < self.n:\n",
    "                output.append(self.sample(1)[0][-1])\n",
    "            else:\n",
    "                ngrams_matching_evidence = ngram_for_decreasing_evidence(output)\n",
    "                if len(ngrams_matching_evidence) == 0:  # append element by frequency\n",
    "                    print(\"fail\")\n",
    "                    output.append(self.sample(1)[0][-1])\n",
    "                    continue\n",
    "                # Now sample among the ngrams that match evidence.\n",
    "                self.weights = [self[ngram] for ngram in ngrams_matching_evidence]\n",
    "                # weighted_sample with replacement returs a single element list containing the sampled ngram\n",
    "                # we select append the last element - word/letter of this ngram to our output\n",
    "                sampled_ngram = weighted_sample_with_replacement(ngrams_matching_evidence, self.weights, 1)\n",
    "                output.append(sampled_ngram[0][-1])\n",
    "        return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_ng_new = NewNgramModel(3,  0.000000000001, list(english_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_ng_new.sample_text(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
