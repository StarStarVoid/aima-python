{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from text import *\n",
    "from decimal import Decimal\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def sentences_from_dataset(file_path):\n",
    "    \"\"\" Read text from a language file and extract sentences. \"\"\"\n",
    "\n",
    "    with open(file_path, 'r') as myfile:\n",
    "        lines=myfile.readlines()\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    for line in lines:\n",
    "        number, sentence = line.split('\\t')\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def clean_text(sentence):\n",
    "    \"\"\" Lowercase the sentence, replace newline character with space and remove punctuation. \"\"\"\n",
    "\n",
    "    sentence = sentence.replace('\\n', ' ')\n",
    "    sentence = sentence.lower()\n",
    "    return ''.join(l for l in sentence if l not in string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract the sentences from our training set for each lanuage and clean them up. ** I am yet to add a description of how the data is stored.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_sentences = sentences_from_dataset('data/eng.txt')\n",
    "french_sentences = sentences_from_dataset('data/fra.txt')\n",
    "ind_sentences = sentences_from_dataset('data/ind.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clean each of these sentences. And join them with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_sentences = ' '.join([clean_text(s) for s in english_sentences])\n",
    "french_sentences = ' '.join([clean_text(s) for s in french_sentences])\n",
    "ind_sentences = ' '.join([clean_text(s) for s in ind_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use some test text and clean it up. The final notebook will have more of these possibly in a dict with keys so we can count how many we got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1_ind = '''Setiap orang berhak mendapat pendidikan. Pendidikan harus gratis, setidak-tidaknya untuk tingkat sekolah rendah dan pendidikan dasar. Pendidikan rendah harus diwajibkan. Pendidikan teknik dan jurusan secara umum harus terbuka bagi semua orang, dan pengajaran tinggi harus secara adil dapat diakses oleh semua orang, berdasarkan kepantasan.'''\n",
    "test2_eng = '''In the traditional sense a hacker is a person who is extremely interested in exploring the things and recondite workings of any computer system. Most often, hackers are the expert programmers.'''\n",
    "\n",
    "test1_ind = clean_text(test1_ind)\n",
    "test2_eng = clean_text(test2_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation 1: Using modified version of NgramTextModel in text.py\n",
    "\n",
    "The current implementation is supposed to be used with words. We can use it for letter n-grams by treating each character as a word. We can create a char n-gram for each language in our training set. I have introduced a slight modification in the constructor to support Laplacian smoothing. You can compare the orignal and the modified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%psource NgramTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New Version\n",
    "\n",
    "class NgramTextModel(CountingProbDist):\n",
    "\n",
    "    \"\"\"This is a discrete probability distribution over n-tuples of words.\n",
    "    You can add, sample or get P[(word1, ..., wordn)]. The method P.samples(n)\n",
    "    builds up an n-word sequence; P.add and P.add_sequence add data.\"\"\"\n",
    "\n",
    "    def __init__(self, n, observation_sequence=[], default=1):\n",
    "        # In addition to the dictionary of n-tuples, cond_prob is a\n",
    "        # mapping from (w1, ..., wn-1) to P(wn | w1, ... wn-1)\n",
    "        CountingProbDist.__init__(self, default=default)\n",
    "        self.n = n\n",
    "        self.cond_prob = defaultdict()\n",
    "        self.add_sequence(observation_sequence)\n",
    "\n",
    "    # __getitem__, top, sample inherited from CountingProbDist\n",
    "    # Note they deal with tuples, not strings, as inputs\n",
    "\n",
    "    def add(self, ngram):\n",
    "        \"\"\"Count 1 for P[(w1, ..., wn)] and for P(wn | (w1, ..., wn-1)\"\"\"\n",
    "        CountingProbDist.add(self, ngram)\n",
    "        if ngram[:-1] not in self.cond_prob:\n",
    "            self.cond_prob[ngram[:-1]] = CountingProbDist()\n",
    "        self.cond_prob[ngram[:-1]].add(ngram[-1])\n",
    "\n",
    "    def add_sequence(self, words):\n",
    "        \"\"\"Add each of the tuple words[i:i+n], using a sliding window.\n",
    "        Prefix some copies of the empty word, '', to make the start work.\"\"\"\n",
    "        n = self.n\n",
    "        words = ['', ] * (n - 1) + words\n",
    "        for i in range(len(words) - n):\n",
    "            self.add(tuple(words[i:i + n]))\n",
    "\n",
    "    def samples(self, nwords):\n",
    "        \"\"\"Build up a random sample of text nwords words long, using\n",
    "        the conditional probability given the n-1 preceding words.\"\"\"\n",
    "        n = self.n\n",
    "        nminus1gram = ('',) * (n-1)\n",
    "        output = []\n",
    "        for i in range(nwords):\n",
    "            if nminus1gram not in self.cond_prob:\n",
    "                nminus1gram = ('',) * (n-1)  # Cannot continue, so restart.\n",
    "            wn = self.cond_prob[nminus1gram].sample()\n",
    "            output.append(wn)\n",
    "            nminus1gram = nminus1gram[1:] + (wn,)\n",
    "        return ' '.join(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_ng = NgramTextModel(3, list(english_sentences), default=1)\n",
    "f_ng = NgramTextModel(3, list(french_sentences),default=1)\n",
    "i_ng = NgramTextModel(3, list(ind_sentences),default=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the most commong occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18862, (' ', 't', 'h')),\n",
       " (15290, ('t', 'h', 'e')),\n",
       " (13334, ('h', 'e', ' ')),\n",
       " (7173, ('e', 'd', ' ')),\n",
       " (6920, ('i', 'n', 'g'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_ng.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are treating P(l) the prior for language as uniform we ignore this term in our calculations. We can evaluate the probability of P(l | test_text) for each language by multiplying the probability of each n-gram in the test_text. We are dealing with tri-grams here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s', 'e', 't'), ('e', 't', 'i'), ('t', 'i', 'a'), ('i', 'a', 'p'), ('a', 'p', ' '), ('p', ' ', 'o'), (' ', 'o', 'r'), ('o', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', 'g'), ('n', 'g', ' '), ('g', ' ', 'b'), (' ', 'b', 'e'), ('b', 'e', 'r'), ('e', 'r', 'h'), ('r', 'h', 'a'), ('h', 'a', 'k'), ('a', 'k', ' '), ('k', ' ', 'm'), (' ', 'm', 'e'), ('m', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'a'), ('d', 'a', 'p'), ('a', 'p', 'a'), ('p', 'a', 't'), ('a', 't', ' '), ('t', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'r'), ('a', 'r', 'u'), ('r', 'u', 's'), ('u', 's', ' '), ('s', ' ', 'g'), (' ', 'g', 'r'), ('g', 'r', 'a'), ('r', 'a', 't'), ('a', 't', 'i'), ('t', 'i', 's'), ('i', 's', ' '), ('s', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 't'), ('e', 't', 'i'), ('t', 'i', 'd'), ('i', 'd', 'a'), ('d', 'a', 'k'), ('a', 'k', 't'), ('k', 't', 'i'), ('t', 'i', 'd'), ('i', 'd', 'a'), ('d', 'a', 'k'), ('a', 'k', 'n'), ('k', 'n', 'y'), ('n', 'y', 'a'), ('y', 'a', ' '), ('a', ' ', 'u'), (' ', 'u', 'n'), ('u', 'n', 't'), ('n', 't', 'u'), ('t', 'u', 'k'), ('u', 'k', ' '), ('k', ' ', 't'), (' ', 't', 'i'), ('t', 'i', 'n'), ('i', 'n', 'g'), ('n', 'g', 'k'), ('g', 'k', 'a'), ('k', 'a', 't'), ('a', 't', ' '), ('t', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'k'), ('e', 'k', 'o'), ('k', 'o', 'l'), ('o', 'l', 'a'), ('l', 'a', 'h'), ('a', 'h', ' '), ('h', ' ', 'r'), (' ', 'r', 'e'), ('r', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'a'), ('d', 'a', 'h'), ('a', 'h', ' '), ('h', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 's'), ('a', 's', 'a'), ('s', 'a', 'r'), ('a', 'r', ' '), ('r', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'r'), (' ', 'r', 'e'), ('r', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'a'), ('d', 'a', 'h'), ('a', 'h', ' '), ('h', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'r'), ('a', 'r', 'u'), ('r', 'u', 's'), ('u', 's', ' '), ('s', ' ', 'd'), (' ', 'd', 'i'), ('d', 'i', 'w'), ('i', 'w', 'a'), ('w', 'a', 'j'), ('a', 'j', 'i'), ('j', 'i', 'b'), ('i', 'b', 'k'), ('b', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 't'), (' ', 't', 'e'), ('t', 'e', 'k'), ('e', 'k', 'n'), ('k', 'n', 'i'), ('n', 'i', 'k'), ('i', 'k', ' '), ('k', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'j'), (' ', 'j', 'u'), ('j', 'u', 'r'), ('u', 'r', 'u'), ('r', 'u', 's'), ('u', 's', 'a'), ('s', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'c'), ('e', 'c', 'a'), ('c', 'a', 'r'), ('a', 'r', 'a'), ('r', 'a', ' '), ('a', ' ', 'u'), (' ', 'u', 'm'), ('u', 'm', 'u'), ('m', 'u', 'm'), ('u', 'm', ' '), ('m', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'r'), ('a', 'r', 'u'), ('r', 'u', 's'), ('u', 's', ' '), ('s', ' ', 't'), (' ', 't', 'e'), ('t', 'e', 'r'), ('e', 'r', 'b'), ('r', 'b', 'u'), ('b', 'u', 'k'), ('u', 'k', 'a'), ('k', 'a', ' '), ('a', ' ', 'b'), (' ', 'b', 'a'), ('b', 'a', 'g'), ('a', 'g', 'i'), ('g', 'i', ' '), ('i', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'm'), ('e', 'm', 'u'), ('m', 'u', 'a'), ('u', 'a', ' '), ('a', ' ', 'o'), (' ', 'o', 'r'), ('o', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', 'g'), ('n', 'g', ' '), ('g', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'g'), ('n', 'g', 'a'), ('g', 'a', 'j'), ('a', 'j', 'a'), ('j', 'a', 'r'), ('a', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 't'), (' ', 't', 'i'), ('t', 'i', 'n'), ('i', 'n', 'g'), ('n', 'g', 'g'), ('g', 'g', 'i'), ('g', 'i', ' '), ('i', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'r'), ('a', 'r', 'u'), ('r', 'u', 's'), ('u', 's', ' '), ('s', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'c'), ('e', 'c', 'a'), ('c', 'a', 'r'), ('a', 'r', 'a'), ('r', 'a', ' '), ('a', ' ', 'a'), (' ', 'a', 'd'), ('a', 'd', 'i'), ('d', 'i', 'l'), ('i', 'l', ' '), ('l', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 'p'), ('a', 'p', 'a'), ('p', 'a', 't'), ('a', 't', ' '), ('t', ' ', 'd'), (' ', 'd', 'i'), ('d', 'i', 'a'), ('i', 'a', 'k'), ('a', 'k', 's'), ('k', 's', 'e'), ('s', 'e', 's'), ('e', 's', ' '), ('s', ' ', 'o'), (' ', 'o', 'l'), ('o', 'l', 'e'), ('l', 'e', 'h'), ('e', 'h', ' '), ('h', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'm'), ('e', 'm', 'u'), ('m', 'u', 'a'), ('u', 'a', ' '), ('a', ' ', 'o'), (' ', 'o', 'r'), ('o', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', 'g'), ('n', 'g', ' '), ('g', ' ', 'b'), (' ', 'b', 'e'), ('b', 'e', 'r'), ('e', 'r', 'd'), ('r', 'd', 'a'), ('d', 'a', 's'), ('a', 's', 'a'), ('s', 'a', 'r'), ('a', 'r', 'k'), ('r', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'k'), (' ', 'k', 'e'), ('k', 'e', 'p'), ('e', 'p', 'a'), ('p', 'a', 'n'), ('a', 'n', 't'), ('n', 't', 'a'), ('t', 'a', 's'), ('a', 's', 'a'), ('s', 'a', 'n'), ('a', 'n')]\n"
     ]
    }
   ],
   "source": [
    "def create_ngrams(text, n):\n",
    "    \"\"\" List of ngram tuples that work well with our NGramTextModel \"\"\"\n",
    "    return [tuple(text[i:i+n]) for i in range(len(text)-1)]\n",
    "\n",
    "test_1_ngrams = create_ngrams(test1_ind, 3)\n",
    "print(test_1_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **P(lang | text)**. We use decimal for precision reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import product\n",
    "\n",
    "prob_lang_dict = dict(\n",
    "english = product([Decimal(e_ng[trigram]) for trigram in test_1_ngrams]),\n",
    "indonesian = product([Decimal(i_ng[trigram]) for trigram in test_1_ngrams]),\n",
    "french = product([Decimal(f_ng[trigram]) for trigram in test_1_ngrams]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to compare these to predict the language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indonesian'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(prob_lang_dict, key=prob_lang_dict.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above workflow nicely wrapped up into a Class. I have left out the procedure of getting the data and cleaning because it may vary depending on the source of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageID:\n",
    "    \"\"\" training_corpus should be a dict of language name as keys and cleaned up strings of text as values. \"\"\"\n",
    "\n",
    "    def __init__(self, training_corpus, n=3, smoothing_factor=1):\n",
    "        self.n = n\n",
    "        self.languages = training_corpus.keys()\n",
    "        self.training_ngram_models = {language: NgramTextModel(n, list(text), smoothing_factor) \n",
    "                                   for language, text in training_corpus.items()}\n",
    "\n",
    "    def create_ngrams(self, text):\n",
    "        \"\"\" List of ngram tuples that work well with our NGramTextModel \"\"\"\n",
    "        return [tuple(text[i:i+self.n]) for i in range(len(text)-1)]\n",
    "    \n",
    "    def calculate_posterior(self, language, test_ngrams):\n",
    "        \"\"\" Posterior sans the P(l) term by taking product of prob of each ngram \"\"\"\n",
    "        return product([Decimal(self.training_ngram_models[language][ngram]) for ngram in test_ngrams])\n",
    "\n",
    "    def predict(self, text):\n",
    "        test_ngrams = self.create_ngrams(text)\n",
    "        prob_lang_dict = {language: self.calculate_posterior(language, test_ngrams) for language in self.languages}\n",
    "        return max(prob_lang_dict, key=prob_lang_dict.get) # Key with max prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor = LanguageID(dict(english=english_sentences, indonesian=ind_sentences, french=french_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test2_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from utils import weighted_sampler, weighted_sample_with_replacement, isclose\n",
    "\n",
    "class DefaultProbDist(dict):\n",
    "    def __init__(self, default_value, mapping=(), **kwargs):\n",
    "        if isinstance(mapping, float):\n",
    "            mapping = {True: mapping, False: 1 - mapping}\n",
    "        self.update(mapping, **kwargs)\n",
    "        self[None] = default_value\n",
    "        self.normalize()\n",
    "       \n",
    "    def __missing__(self, key): \n",
    "        \"If we haven't seen key before, use default_value as the probability and re-normalize.\"\n",
    "        self[key] = self.default_value\n",
    "        self.normalize()\n",
    "        return self[key]\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Make sure the probabilities of all values sum to 1.\n",
    "        Returns the normalized distribution.\n",
    "        Raises a ZeroDivisionError if the sum of the values is 0.\"\"\"\n",
    "        total = sum(self.values())\n",
    "        if not isclose(total, 1.0):\n",
    "            for key in self:\n",
    "                self[key] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New NGram Model with better sampling\n",
    "\n",
    "class NewNgramModel(DefaultProbDist):\n",
    "    \"\"\" Avoids Counting Prob Dist. Uses filtering instead of creating a cond prob dist so \n",
    "        as to handle cases better when nothing is found meeting the evidence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, default_value, observation_sequence=[]):\n",
    "        self.n = n\n",
    "        self.element_counts = Counter(observation_sequence) \n",
    "        self.ngram_counts = self.generate_ngram_counts(observation_sequence, n)\n",
    "        super().__init__(default_value, self.ngram_counts)\n",
    "    \n",
    "    def generate_ngram_counts(self, observation_sequence, n):\n",
    "        return Counter(tuple(observation_sequence[i:i+n]) for i in range(len(observation_sequence)-1))\n",
    "    \n",
    "    def sample(self, sample_length):\n",
    "        \"\"\" If sample_length is less than n we sample according to frequency of words/letters (elements)\n",
    "            In case the last n-1 elements which act as evidence are not found we again sample the new element\n",
    "            in output decreasing the evidence to support an n-1 gram.\n",
    "        \"\"\"\n",
    "        def ngram_for_decreasing_evidence(e_size, output):\n",
    "            e_size = self.n  # Evidence Size\n",
    "            ngrams_matching_evidence = []\n",
    "            while len(ngrams_matching_evidence) == 0:  # Decrease evidence to support a smaller ngram of e_size if not found\n",
    "                evidence = output[-(e_size-1):]  # Last elements from currrently generated output\n",
    "                \n",
    "                # Next we filter matching the evidence with last e_size characters of the ngrams sans the last element \n",
    "                # (This is the value of our query)\n",
    "                ngrams_matching_evidence = [ngram for ngram in self if ngram is not None and list(ngram[-1 * e_size :-1]) == evidence]\n",
    "                e_size -= 1\n",
    "                if e_size == 1:\n",
    "                    return None  # Return none when no ngram supports evidence.\n",
    "            return ngrams_matching_evidence\n",
    "\n",
    "        output = []\n",
    "        while len(output) != sample_length:\n",
    "            if len(output) < self.n:\n",
    "                # weighted_sample with replacement returs a single element list containing a single element - word/letter\n",
    "                output.append(weighted_sample_with_replacement(list(self.element_counts.keys()),\n",
    "                                                               list(self.element_counts.values()), 1)[0])\n",
    "            else:\n",
    "                ngrams_matching_evidence = ngram_for_decreasing_evidence(self.n, output)\n",
    "                if ngrams_matching_evidence is None:\n",
    "                # weighted_sample with replacement returs a single element list containing a single element - word/letter\n",
    "                    output.append(weighted_sample_with_replacement(list(self.element_counts.keys()),\n",
    "                                                                   list(self.element_counts.values()), 1)[0])\n",
    "                    continue\n",
    "                self.weights = [self[ngram] for ngram in ngrams_matching_evidence]\n",
    "                # weighted_sample with replacement returs a single element list containing the sampled ngram\n",
    "                # we select append the last element - word/letter of this ngram to our output\n",
    "                sampled_ngram = weighted_sample_with_replacement(ngrams_matching_evidence, self.weights, 1)\n",
    "                output.append(sampled_ngram[0][-1])\n",
    "        return ''.join(output)\n",
    "    \n",
    "    def most_common(self, i=5):\n",
    "        return self.ngram_counts.most_common(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_ng_new = NewNgramModel(3,  0.000000000001, list(english_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fsited the bridunt  of of the  the the fordippeth fine my 16 on ther sithatiminisch and nesch theyea'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_ng_new.sample(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
