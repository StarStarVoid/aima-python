{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from text import *\n",
    "from decimal import Decimal\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def sentences_from_dataset(file_path):\n",
    "    \"\"\" Read text from a language file and extract sentences. \"\"\"\n",
    "\n",
    "    with open(file_path, 'r') as myfile:\n",
    "        lines=myfile.readlines()\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    for line in lines:\n",
    "        number, sentence = line.split('\\t')\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def clean_text(sentence):\n",
    "    \"\"\" Lowercase the sentence, replace newline character with space and remove punctuation. \"\"\"\n",
    "\n",
    "    sentence = sentence.replace('\\n', ' ')\n",
    "    sentence = sentence.lower()\n",
    "    return ''.join(l for l in sentence if l not in string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract the sentences from our training set for each lanuage and clean them up. ** I am yet to add a description of how the data is stored.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_sentences = sentences_from_dataset('data/eng.txt')\n",
    "french_sentences = sentences_from_dataset('data/fra.txt')\n",
    "ind_sentences = sentences_from_dataset('data/ind.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clean each of these sentences. And join them with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_sentences = ' '.join([clean_text(s) for s in english_sentences])\n",
    "french_sentences = ' '.join([clean_text(s) for s in french_sentences])\n",
    "ind_sentences = ' '.join([clean_text(s) for s in ind_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use some test text and clean it up. The final notebook will have more of these possibly in a dict with keys so we can count how many we got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1_ind = '''Setiap orang berhak mendapat pendidikan. Pendidikan harus gratis, setidak-tidaknya untuk tingkat sekolah rendah dan pendidikan dasar. Pendidikan rendah harus diwajibkan. Pendidikan teknik dan jurusan secara umum harus terbuka bagi semua orang, dan pengajaran tinggi harus secara adil dapat diakses oleh semua orang, berdasarkan kepantasan.'''\n",
    "test2_eng = '''In the traditional sense a hacker is a person who is extremely interested in exploring the things and recondite workings of any computer system. Most often, hackers are the expert programmers.'''\n",
    "\n",
    "test1_ind = clean_text(test1_ind)\n",
    "test2_eng = clean_text(test2_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation 1: Using modified version of NgramTextModel in text.py\n",
    "\n",
    "The current implementation is supposed to be used with words. We can use it for letter n-grams by treating each character as a word. We can create a char n-gram for each language in our training set. I have introduced a slight modification in the constructor to support Laplacian smoothing. You can compare the orignal and the modified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%psource NgramTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New Version\n",
    "\n",
    "class NgramTextModel(CountingProbDist):\n",
    "\n",
    "    \"\"\"This is a discrete probability distribution over n-tuples of words.\n",
    "    You can add, sample or get P[(word1, ..., wordn)]. The method P.samples(n)\n",
    "    builds up an n-word sequence; P.add and P.add_sequence add data.\"\"\"\n",
    "\n",
    "    def __init__(self, n, observation_sequence=[], default=1):\n",
    "        # In addition to the dictionary of n-tuples, cond_prob is a\n",
    "        # mapping from (w1, ..., wn-1) to P(wn | w1, ... wn-1)\n",
    "        CountingProbDist.__init__(self, default=default)\n",
    "        self.n = n\n",
    "        self.cond_prob = defaultdict()\n",
    "        self.add_sequence(observation_sequence)\n",
    "\n",
    "    # __getitem__, top, sample inherited from CountingProbDist\n",
    "    # Note they deal with tuples, not strings, as inputs\n",
    "\n",
    "    def add(self, ngram):\n",
    "        \"\"\"Count 1 for P[(w1, ..., wn)] and for P(wn | (w1, ..., wn-1)\"\"\"\n",
    "        CountingProbDist.add(self, ngram)\n",
    "        if ngram[:-1] not in self.cond_prob:\n",
    "            self.cond_prob[ngram[:-1]] = CountingProbDist()\n",
    "        self.cond_prob[ngram[:-1]].add(ngram[-1])\n",
    "\n",
    "    def add_sequence(self, words):\n",
    "        \"\"\"Add each of the tuple words[i:i+n], using a sliding window.\n",
    "        Prefix some copies of the empty word, '', to make the start work.\"\"\"\n",
    "        n = self.n\n",
    "        words = ['', ] * (n - 1) + words\n",
    "        for i in range(len(words) - n):\n",
    "            self.add(tuple(words[i:i + n]))\n",
    "\n",
    "    def samples(self, nwords):\n",
    "        \"\"\"Build up a random sample of text nwords words long, using\n",
    "        the conditional probability given the n-1 preceding words.\"\"\"\n",
    "        n = self.n\n",
    "        nminus1gram = ('',) * (n-1)\n",
    "        output = []\n",
    "        for i in range(nwords):\n",
    "            if nminus1gram not in self.cond_prob:\n",
    "                nminus1gram = ('',) * (n-1)  # Cannot continue, so restart.\n",
    "            wn = self.cond_prob[nminus1gram].sample()\n",
    "            output.append(wn)\n",
    "            nminus1gram = nminus1gram[1:] + (wn,)\n",
    "        return ' '.join(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_ng = NgramTextModel(3, list(english_sentences), default=1)\n",
    "f_ng = NgramTextModel(3, list(french_sentences),default=1)\n",
    "i_ng = NgramTextModel(3, list(ind_sentences),default=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the most commong occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18862, (' ', 't', 'h')),\n",
       " (15290, ('t', 'h', 'e')),\n",
       " (13334, ('h', 'e', ' ')),\n",
       " (7173, ('e', 'd', ' ')),\n",
       " (6920, ('i', 'n', 'g'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_ng.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are treating P(l) the prior for language as uniform we ignore this term in our calculations. We can evaluate the probability of P(l | test_text) for each language by multiplying the probability of each n-gram in the test_text. We are dealing with tri-grams here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s', 'e', 't'), ('e', 't', 'i'), ('t', 'i', 'a'), ('i', 'a', 'p'), ('a', 'p', ' '), ('p', ' ', 'o'), (' ', 'o', 'r'), ('o', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', 'g'), ('n', 'g', ' '), ('g', ' ', 'b'), (' ', 'b', 'e'), ('b', 'e', 'r'), ('e', 'r', 'h'), ('r', 'h', 'a'), ('h', 'a', 'k'), ('a', 'k', ' '), ('k', ' ', 'm'), (' ', 'm', 'e'), ('m', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'a'), ('d', 'a', 'p'), ('a', 'p', 'a'), ('p', 'a', 't'), ('a', 't', ' '), ('t', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'r'), ('a', 'r', 'u'), ('r', 'u', 's'), ('u', 's', ' '), ('s', ' ', 'g'), (' ', 'g', 'r'), ('g', 'r', 'a'), ('r', 'a', 't'), ('a', 't', 'i'), ('t', 'i', 's'), ('i', 's', ' '), ('s', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 't'), ('e', 't', 'i'), ('t', 'i', 'd'), ('i', 'd', 'a'), ('d', 'a', 'k'), ('a', 'k', 't'), ('k', 't', 'i'), ('t', 'i', 'd'), ('i', 'd', 'a'), ('d', 'a', 'k'), ('a', 'k', 'n'), ('k', 'n', 'y'), ('n', 'y', 'a'), ('y', 'a', ' '), ('a', ' ', 'u'), (' ', 'u', 'n'), ('u', 'n', 't'), ('n', 't', 'u'), ('t', 'u', 'k'), ('u', 'k', ' '), ('k', ' ', 't'), (' ', 't', 'i'), ('t', 'i', 'n'), ('i', 'n', 'g'), ('n', 'g', 'k'), ('g', 'k', 'a'), ('k', 'a', 't'), ('a', 't', ' '), ('t', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'k'), ('e', 'k', 'o'), ('k', 'o', 'l'), ('o', 'l', 'a'), ('l', 'a', 'h'), ('a', 'h', ' '), ('h', ' ', 'r'), (' ', 'r', 'e'), ('r', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'a'), ('d', 'a', 'h'), ('a', 'h', ' '), ('h', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 's'), ('a', 's', 'a'), ('s', 'a', 'r'), ('a', 'r', ' '), ('r', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'r'), (' ', 'r', 'e'), ('r', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'a'), ('d', 'a', 'h'), ('a', 'h', ' '), ('h', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'r'), ('a', 'r', 'u'), ('r', 'u', 's'), ('u', 's', ' '), ('s', ' ', 'd'), (' ', 'd', 'i'), ('d', 'i', 'w'), ('i', 'w', 'a'), ('w', 'a', 'j'), ('a', 'j', 'i'), ('j', 'i', 'b'), ('i', 'b', 'k'), ('b', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'i'), ('d', 'i', 'k'), ('i', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 't'), (' ', 't', 'e'), ('t', 'e', 'k'), ('e', 'k', 'n'), ('k', 'n', 'i'), ('n', 'i', 'k'), ('i', 'k', ' '), ('k', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'j'), (' ', 'j', 'u'), ('j', 'u', 'r'), ('u', 'r', 'u'), ('r', 'u', 's'), ('u', 's', 'a'), ('s', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'c'), ('e', 'c', 'a'), ('c', 'a', 'r'), ('a', 'r', 'a'), ('r', 'a', ' '), ('a', ' ', 'u'), (' ', 'u', 'm'), ('u', 'm', 'u'), ('m', 'u', 'm'), ('u', 'm', ' '), ('m', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'r'), ('a', 'r', 'u'), ('r', 'u', 's'), ('u', 's', ' '), ('s', ' ', 't'), (' ', 't', 'e'), ('t', 'e', 'r'), ('e', 'r', 'b'), ('r', 'b', 'u'), ('b', 'u', 'k'), ('u', 'k', 'a'), ('k', 'a', ' '), ('a', ' ', 'b'), (' ', 'b', 'a'), ('b', 'a', 'g'), ('a', 'g', 'i'), ('g', 'i', ' '), ('i', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'm'), ('e', 'm', 'u'), ('m', 'u', 'a'), ('u', 'a', ' '), ('a', ' ', 'o'), (' ', 'o', 'r'), ('o', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', 'g'), ('n', 'g', ' '), ('g', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'p'), (' ', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 'g'), ('n', 'g', 'a'), ('g', 'a', 'j'), ('a', 'j', 'a'), ('j', 'a', 'r'), ('a', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 't'), (' ', 't', 'i'), ('t', 'i', 'n'), ('i', 'n', 'g'), ('n', 'g', 'g'), ('g', 'g', 'i'), ('g', 'i', ' '), ('i', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'r'), ('a', 'r', 'u'), ('r', 'u', 's'), ('u', 's', ' '), ('s', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'c'), ('e', 'c', 'a'), ('c', 'a', 'r'), ('a', 'r', 'a'), ('r', 'a', ' '), ('a', ' ', 'a'), (' ', 'a', 'd'), ('a', 'd', 'i'), ('d', 'i', 'l'), ('i', 'l', ' '), ('l', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 'p'), ('a', 'p', 'a'), ('p', 'a', 't'), ('a', 't', ' '), ('t', ' ', 'd'), (' ', 'd', 'i'), ('d', 'i', 'a'), ('i', 'a', 'k'), ('a', 'k', 's'), ('k', 's', 'e'), ('s', 'e', 's'), ('e', 's', ' '), ('s', ' ', 'o'), (' ', 'o', 'l'), ('o', 'l', 'e'), ('l', 'e', 'h'), ('e', 'h', ' '), ('h', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'm'), ('e', 'm', 'u'), ('m', 'u', 'a'), ('u', 'a', ' '), ('a', ' ', 'o'), (' ', 'o', 'r'), ('o', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', 'g'), ('n', 'g', ' '), ('g', ' ', 'b'), (' ', 'b', 'e'), ('b', 'e', 'r'), ('e', 'r', 'd'), ('r', 'd', 'a'), ('d', 'a', 's'), ('a', 's', 'a'), ('s', 'a', 'r'), ('a', 'r', 'k'), ('r', 'k', 'a'), ('k', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'k'), (' ', 'k', 'e'), ('k', 'e', 'p'), ('e', 'p', 'a'), ('p', 'a', 'n'), ('a', 'n', 't'), ('n', 't', 'a'), ('t', 'a', 's'), ('a', 's', 'a'), ('s', 'a', 'n'), ('a', 'n')]\n"
     ]
    }
   ],
   "source": [
    "def create_ngrams(text, n):\n",
    "    \"\"\" List of ngram tuples that work well with our NGramTextModel \"\"\"\n",
    "    return [tuple(text[i:i+n]) for i in range(len(text)-1)]\n",
    "\n",
    "test_1_ngrams = create_ngrams(test1_ind, 3)\n",
    "print(test_1_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **P(lang | text)**. We use decimal for precision reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import product\n",
    "\n",
    "prob_lang_dict = dict(\n",
    "english = product([Decimal(e_ng[trigram]) for trigram in test_1_ngrams]),\n",
    "indonesian = product([Decimal(i_ng[trigram]) for trigram in test_1_ngrams]),\n",
    "french = product([Decimal(f_ng[trigram]) for trigram in test_1_ngrams]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to compare these to predict the language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indonesian'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(prob_lang_dict, key=prob_lang_dict.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above workflow nicely wrapped up into a Class. I have left out the procedure of getting the data and cleaning because it may vary depending on the source of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageID:\n",
    "    \"\"\" training_corpus should be a dict of language name as keys and cleaned up strings of text as values. \"\"\"\n",
    "\n",
    "    def __init__(self, training_corpus, n=3, smoothing_factor=1):\n",
    "        self.n = n\n",
    "        self.languages = training_corpus.keys()\n",
    "        self.training_ngram_models = {language: NgramTextModel(n, list(text), smoothing_factor) \n",
    "                                   for language, text in training_corpus.items()}\n",
    "\n",
    "    def create_ngrams(self, text):\n",
    "        \"\"\" List of ngram tuples that work well with our NGramTextModel \"\"\"\n",
    "        return [tuple(text[i:i+self.n]) for i in range(len(text)-1)]\n",
    "    \n",
    "    def calculate_posterior(self, language, test_ngrams):\n",
    "        \"\"\" Posterior sans the P(l) term by taking product of prob of each ngram \"\"\"\n",
    "        return product([Decimal(self.training_ngram_models[language][ngram]) for ngram in test_ngrams])\n",
    "\n",
    "    def predict(self, text):\n",
    "        test_ngrams = self.create_ngrams(text)\n",
    "        prob_lang_dict = {language: self.calculate_posterior(language, test_ngrams) for language in self.languages}\n",
    "        return max(prob_lang_dict, key=prob_lang_dict.get) # Key with max prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor = LanguageID(dict(english=english_sentences, indonesian=ind_sentences, french=french_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test2_eng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
